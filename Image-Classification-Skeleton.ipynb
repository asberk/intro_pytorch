{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From `Example-Notebook`\n",
    "\n",
    "Nothing new in this section. Just repeating the code.\n",
    "\n",
    "**Exercise:** clean up these notebooks by moving this section to its own module, and then importing it in both this note book and `Example-Notebook-Skeleton`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerDenseNet(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, out_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, out_features)\n",
    "        self.do = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = torch.relu(self.fc1(inputs))\n",
    "        outputs = self.fc2(self.do(outputs))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_and_val_functions(model, criterion, optimizer):\n",
    "    def train_step(batch):\n",
    "        model.train()\n",
    "        inputs, targets = batch\n",
    "        y_logits = model(inputs)\n",
    "        loss = criterion(y_logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred = torch.max(y_logits, 1).indices\n",
    "        loss_value = loss.item()\n",
    "        correct = (y_pred == targets.detach())\n",
    "        return {'loss': loss_value, \n",
    "                'num_correct': correct.long().sum().numpy(), \n",
    "                'batch_accuracy': correct.float().mean().numpy(), \n",
    "                'batch_size': correct.numpy().size}\n",
    "\n",
    "    def eval_step(batch):\n",
    "        model.eval()\n",
    "        inputs, targets = batch\n",
    "        y_logits = model(inputs)\n",
    "        loss = criterion(y_logits, targets)\n",
    "        y_pred = torch.max(y_logits, 1).indices\n",
    "        loss_value = loss.item()\n",
    "        correct = (y_pred == targets.detach())\n",
    "        return {'loss': loss_value, \n",
    "                'num_correct': correct.long().sum().numpy(), \n",
    "                'batch_accuracy': correct.float().mean().numpy(), \n",
    "                'batch_size': correct.numpy().size}\n",
    "    return train_step, eval_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: MNIST Classification with Dense Network\n",
    "\n",
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "# ...\n",
    "on_load_tsfm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "train_digits = datasets.MNIST(DATA_DIR, train=True, transform=on_load_tsfm)\n",
    "test_digits = datasets.MNIST(DATA_DIR, train=False, transform=on_load_tsfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_digits.targets.size(0)\n",
    "test_size = test_digits.targets.size(0)\n",
    "print('number of images')\n",
    "print(f'train: {train_size}')\n",
    "print(f' test: {test_size}')\n",
    "\n",
    "\n",
    "\n",
    "# define val and eval_train size\n",
    "val_size = test_size\n",
    "eval_train_size = val_size\n",
    "\n",
    "# subset train_digits to form val and eval_train data\n",
    "val_idx = np.random.choice(train_size, val_size, replace=False)\n",
    "new_train_idx = np.setdiff1d(range(train_size), val_idx)\n",
    "\n",
    "val_digits = Subset(train_digits, val_idx)\n",
    "train_digits = Subset(train_digits, new_train_idx)\n",
    "train_size = len(train_digits)\n",
    "\n",
    "eval_train_idx = np.random.choice(train_size, eval_train_size, replace=False)\n",
    "eval_train_digits = Subset(train_digits, eval_train_idx)\n",
    "\n",
    "# create dataloaders\n",
    "dl = {\n",
    "    'train': DataLoader(train_digits, batch_size=16, shuffle=True),\n",
    "    'eval_train': DataLoader(eval_train_digits, batch_size=128, shuffle=True),\n",
    "    'val': DataLoader(val_digits, batch_size=128, shuffle=True),\n",
    "    'test': DataLoader(test_digits, batch_size=128, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model called `model` with 10 hidden layers.\n",
    "# ...\n",
    "\n",
    "# Print out a summary of the model using `summary`\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define objects for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the criterion to be cross-entropy loss\n",
    "# ...\n",
    "criterion = \n",
    "\n",
    "# Use stochastic gradient descent with:\n",
    "# - learning rate of 1e-4, \n",
    "# - momentum=.9\n",
    "# - nesterov=True\n",
    "# ...\n",
    "optimizer = \n",
    "\n",
    "# Instantiate an exponential learning rate scheduler\n",
    "# with annealing rate of .9\n",
    "# ...\n",
    "scheduler = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train and eval stepping functions\n",
    "# ...\n",
    "train_step, eval_step = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded images will be square, but we \n",
    "# want vectors for our dense net.\n",
    "_flatten = nn.Flatten()\n",
    "def flatten(batch):\n",
    "    images, labels = batch\n",
    "    images = _flatten(images)\n",
    "    batch = (images, labels)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write the code required for 25 epochs of training. After each epoch has finished, evaluate the model on the `eval_train` dataloader and the `val` dataloader. Print out the epoch's average accuracy and loss for the two loaders. \n",
    "\n",
    "*Hint:* Use `flatten`, defined above to ensure that the (square) images are correctly formatted for the dense network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine network output\n",
    "\n",
    "For which digits does it do well? For which does it have a hard time? \n",
    "\n",
    "**Exercise:** Make a confusion matrix for the predictions on the test data. Either print it out or visualize it. \n",
    "\n",
    "*Hint:* Evaluate the entire test data `Tensor` at once (rather than looping through the `DataLoader` we created). Convert the resulting logits into to a numpy array and work with those. \n",
    "\n",
    "*Hint:* `sklearn` has a `confusion_matrix` function. \n",
    "\n",
    "*Hint:* `matplotlib`'s `matshow` is one way of visualizing the confusion matrix\n",
    "\n",
    "**Exercise:** Using the test logits and labels obtained above, write a function that approximates the [AUROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: MNIST classification with CNN\n",
    "\n",
    "## Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, input_size, n_filters, num_classes):\n",
    "        super().__init__()\n",
    "        # n_filters; 3x3 convolution; padding=1\n",
    "        self.c1 = nn.Conv2d(input_size[0], n_filters, kernel_size=(3,3), padding=1)\n",
    "        # batch normalization\n",
    "        self.bn1 = nn.BatchNorm2d(n_filters)\n",
    "        # 2x2 max pool\n",
    "        self.mp1 = nn.MaxPool2d((2,2))\n",
    "        # use n_filters again; 3x3; padding=0\n",
    "        self.c2 = \n",
    "        # batch normalization\n",
    "        self.bn2 = \n",
    "        # 2x2 max-pooling\n",
    "        self.mp2 = \n",
    "        # number of input features to the final layer\n",
    "        fc_in = int(n_filters * (input_size[1] - 4) * (input_size[2] - 4)/16)\n",
    "        # For reshaping the input to the final layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        # The final layer\n",
    "        self.fc = nn.Linear(fc_in, num_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Convolve -> Batch Normalization -> ReLU -> MaxPool\n",
    "        outputs = \n",
    "        # Convolve -> Batch Normalization -> ReLU -> MaxPool\n",
    "        outputs = \n",
    "        # Fully connected\n",
    "        outputs = \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SmallCNN with 8 filters and the appropriate number of classes\n",
    "# ...\n",
    "model = \n",
    "\n",
    "# Print out a summary\n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = # cross entropy\n",
    "optimizer = # SGD, same as before\n",
    "scheduler = # exponential annealing, same as before\n",
    "train_step, eval_step = # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use above code to write a 25 epoch training loop\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the results\n",
    "\n",
    "**Exercise:** Implement this section on [Google Colab](colab.research.google.com) where it will run much faster if you use a GPU-enabled notebook.\n",
    "\n",
    "**Exercise:** Compare the classification results of the CNN to those for the dense net. Which does better?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
